<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Word2vec(Google 2013)</title>
      <link href="/2022/05/24/word2vec/"/>
      <url>/2022/05/24/word2vec/</url>
      
        <content type="html"><![CDATA[<p><strong><font color="LightCoral">paper</font></strong>:</p><ol><li><a href="https://arxiv.org/abs/1301.3781">https://arxiv.org/abs/1301.3781</a><br>explain: <a href="https://so.csdn.net/so/search?q=word2vec&amp;spm=1001.2101.3001.7020">word2vec</a>的奠基性论文之一，由Google的Tomas Mikolov提出。该论文提出了CBOW和Skip-gram两种word2vec模型结构。</li><li><a href="https://proceedings.neurips.cc/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html">https://proceedings.neurips.cc/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html</a><br>explain: word2vec的另一篇奠基性论文。详细描述了Skip-gram模型，包括模型的具体形式和Hierarchical Softmax、Negative Sampling两种可行的训练方法。</li></ol><h1 id="词表征"><a href="#词表征" class="headerlink" title="词表征"></a>词表征</h1><ol><li>$one-hot$ 表征(很难表达相似度)<ol><li>维度太大</li><li>每个元素是离散的的，要不就是0，要不就是1</li><li>$local$ 只有一位起决定作用</li></ol></li><li>分布式表示<ol><li>维度相对较低 $d \ll \left|V\right|$</li><li>每个元素不再是离散的，可以是不同的实数</li><li>并不是只有一位起作用，联合表达一个单词</li></ol></li></ol><p>Word2vec就是一种分布式表示的方法</p><h1 id="Word2vec"><a href="#Word2vec" class="headerlink" title="Word2vec"></a>Word2vec</h1><p>Training Data: corpus</p><script type="math/tex; mode=display">[\omega_1,\omega_2,\cdots,\omega_{t-1},\omega_t,\omega_{t+1},\cdots,\omega_T]</script><p>Target: 词的分布式表示</p><hr><p><strong><font color="DeepPink">接下来的分析过程是基于$skip-gram$,会提到三个假设：</font></strong></p><p>首先我们来看联合概率分布，联合概率表示为包含多个条件并且所有的条件都同时成立的概率，记作 $P(X=a,Y=b)$或 $P(a,b)$，有的书上也习惯记作$P(ab)$。根据公式$P(X,Y)=P(X|Y)\times P(Y)$,我们从词库中随便选择一个词，那么整个词库的概率表示应该如下所示，<strong><font color="DarkViolet">因为后面的计算会很复杂，这里为了方便计算，$\triangle$表示假设窗口大小是2C</font></strong></p><script type="math/tex; mode=display">P(\omega_{1:T})= P(\omega_t) \cdot P(context(\omega_t)|\omega_t)\triangleq P(\omega_t) \cdot P(\omega_{t-c:t-1},\omega_{t+1:t+c}|\omega_t)</script><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">补充：“似然性”和“概率”（或然性）有明确的区分：概率，用于在已知一些参数的情况下，预测接下来在观测上所得到的结果；似然性，则是用于在已知某些观测所得到的结果时，对有关事物之性质的参数进行估值，也就是说已观察到某事件后，对相关母数进行猜测。</span><br></pre></td></tr></table></figure><p><strong>似然函数（</strong>英语：$likelihood function$）是一种关于<a href="https://zh.m.wikipedia.org/wiki/统计模型">统计模型</a>中的<a href="https://zh.m.wikipedia.org/wiki/母數">参数</a>的<a href="https://zh.m.wikipedia.org/wiki/函数">函数</a>，表示模型参数中的<strong>似然性</strong></p><p>所以我们从上式可以写出$likelihood$：</p><script type="math/tex; mode=display">P(\omega_t) \cdot P(\omega_{t-c:t-1},\omega_{t+1:t+c}|\omega_t)</script><p>又因为我们不太关注$P(\omega_t)$,因为是随机取的，而且有$T$个，所以我们更关注后面的条件概率</p><p>所以$conditional likelihood$可以表示为:</p><script type="math/tex; mode=display">\prod_{t=1}^{T}P(\omega_{t-c:t-1},\omega_{t+1:t+c}|\omega_t)</script><p> <strong><font color="DarkViolet">这里主要假设每个条件概率之间是相互独立的</font></strong></p><p>为了化简成更简单的形式，采用$\log$，则$average conditional likelihood$可以如下图表示：</p><script type="math/tex; mode=display">\frac{1}{T} \sum_{t=1}^{T}\log P(\omega_{t-c:t-1},\omega_{t+1:t+c}|\omega_t)</script><p><strong><font color="DarkViolet">这里我们做第三个假设，假设内部$P(\omega_{t+i}|\omega_t)$是独立同分布的</font></strong></p><p>于是继续化简上式为</p><script type="math/tex; mode=display">\frac{1}{T} \sum_{t=1}^{T}\log \prod_{i \in[-c,0) \cup (0,c]  }P(\omega_{t+i}|\omega_t) =  \frac{1}{T} \sum_{t=1}^{T} \sum_{i \in[-c,0) \cup (0,c]  } \log P(\omega_{t+i}|\omega_t)</script><p>我们转换到现在可以看出主要关注给定一个中心词，该中心词窗口内的词的概率就可以，<strong><font color="DarkViolet">注意，这里窗口内的词都是独立的，不存在顺序问题(比如要先出现$\omega_{t}$再出现$\omega_{t+1}$)，这样的表示已经忽略了次序，这里都是独立的。</font></strong></p><hr><p><strong><font color="LightCoral">当然，从上面的分析我们暂时看不出来和求词表分布式表示有啥关系？接下来我们探索这个问题</font></strong></p><p>首先我们从输入输出和模型的角度来分析，先看下面这张图</p><p><img src="/2022/05/24/word2vec/image-20220524213551921.png" alt="image-20220524213551921"></p><p>首先我们选取一个中心词$\omega_i$，用$one-hot$的表示输入的话则是一个$\mathbb R^{|V| \times 1}$的向量，其中$|V|$是词表中所有单词的数量，这个向量只有在第$i$位是1，其他位全都为0。参数矩阵$W \in \mathbb R^{|V| \times d}$和$U \in \mathbb R^{d \times |V|} $，<strong><font color="DarkViolet">注意这里的$W$矩阵没有使用激活层，是线性的，为了计算简单，如果用矩阵相乘的方式，那么用$\mathbb R^{|V| \times 1}$的向量转置以后得到的$\mathbb R^{ 1 \times |V| }$向量作为输入，在模型的输出也会得到一个$\mathbb R^{ 1 \times |V| }$的向量</font></strong>，这里我们要特别理解模型的输出代表什么，如下所示：</p><script type="math/tex; mode=display">\mathbb R^{ 1 \times |V| } \stackrel{每一位的表示}\longrightarrow \left\{ P(\omega_1|\omega_i),P(\omega_2|\omega_i),P(\omega_3|\omega_i),\cdots,P(\omega_{|V|}|\omega_i)\right\}</script><p>然后使用softmax将所有概率值放在$(0，1]$区间当中，即对于中心词$\omega_{i}$，所预测的其他所有词的概率，令$softmax$之后的值为，</p><script type="math/tex; mode=display">\left\{ \hat P(\omega_1|\omega_i),\hat P(\omega_2|\omega_i),\hat P(\omega_3|\omega_i),\cdots,\hat P(\omega_{|V|}|\omega_i)\right\}</script><p>对于中心词$\omega_{i}$，所预测的其他所有词的概率之和为1：</p><script type="math/tex; mode=display">\sum_{t=1}^{|V|} \hat P(\omega_t|\omega_i)=1</script><p><img src="/2022/05/24/word2vec/image-20220524233941683.png" alt="image-20220524233941683"></p><p><br></p><p>当然我们需要在$\omega_{i}$长度为$2c$的窗口中每个上下文一个位置所有的概率值，注意下图对应的$W^{‘}$参数矩阵就是上图的$U$矩阵(注意，不是多个不同的U矩阵，都是同一个，这里为了形象化不同的上下文位置拆开了)，这里我理解假如一共有5个词，s上下文窗口有两个词对应的索引分别式1和3，则所代表的$\hat y_{target}=[0,1,0,1,0]$，在损失函数优化的时候应该用$\hat y_{target}-y_{output}$合在一起算，下面的分开算只是为了好理解，其前面的c个词和后面的c个词作为了$Skip-Gram$模型的输出,，期望这些词的$softmax$概率比其他的词大。</p><p><br></p><p><img src="/2022/05/24/word2vec/20200831235730940.png" alt></p><p><br></p><p>最后我们讨论损失函数，我们已经有了上面的似然函数，则损失函数的优化可以如下表示，因为我们要最大化中心词和上下文之间的联系概率，所以要</p><script type="math/tex; mode=display">\max \frac{1}{T} \sum_{t=1}^{T} \sum_{i \in[-c,0) \cup (0,c]  } \log P(\omega_{t+i}|\omega_t)</script><p>对上式我们取负数，则变成了最小化：</p><script type="math/tex; mode=display">\min -\frac{1}{T} \sum_{t=1}^{T} \sum_{i \in[-c,0) \cup (0,c]  } \log P(\omega_{t+i}|\omega_t)</script><p>所以损失函数即为：</p><script type="math/tex; mode=display">J(\theta)=-\frac{1}{T} \sum_{t=1}^{T} \sum_{i \in[-c,0) \cup (0,c]  } \log P(\omega_{t+i}|\omega_t)</script><p><strong><font color="Crimson">那最后该怎么得到所有词的向量呢？我们在训练的过程中不断的更新$W$和$U$的参数矩阵，在训练完成之后，用$one-hot$向量和训练好的$W$矩阵相乘就是得到的词向量了，也就是词的embedding！！！（注意，不需要$U$矩阵）</font></strong></p><p>最后</p><p><strong><font color="LightCoral">这里skip-gram的介绍就完成了，CBOW的思想类似</font></strong></p><h1 id="两个优化方法"><a href="#两个优化方法" class="headerlink" title="两个优化方法"></a>两个优化方法</h1><h2 id="Hierarchical-Softmax"><a href="#Hierarchical-Softmax" class="headerlink" title="Hierarchical Softmax"></a>Hierarchical Softmax</h2><p><strong><font color="MediumBlue">思考：如果词库特别大，也就是$|V|$特别大，那在最后计算$softmax$的计算量也会非常大，怎么办呢？</font></strong></p><p>该方法最后一层的长度不设置成$|V|$大小，而是设置成一棵树的节点数的长度，这里首先要了解$Huffman$树的构造方法和$Huffman$编码。</p><p><strong><font color="DarkViolet">这里看这个图，非常容易理解，按照词频进行节点的融合，越靠近根节点的词频越大，黄色节点是在构成树的过程当中添加的节点，注意，所有词都在叶子节点上，但非叶子节点上也要定义和叶子节点等长的向量，为了方便计算</font></strong></p><p><img src="/2022/05/24/word2vec/image-20220525010308984.png" alt="image-20220525010308984"></p><p><br></p><p><strong><font color="LightSeaGreen" size="4">$CBOW$</font></strong></p><p><br></p><p>在得到了上面的图之后，我们就可以来计算概率，首先以$CBOW$为例子，将输入层的$2c$个词向量累加求和，即</p><script type="math/tex; mode=display">X_w=\sum_{i=1}^{2c}v(context(w_i)) \quad v(context(w_i)\in \mathbb R^{m}</script><p> <strong><font color="DarkViolet">这里一定要注意，一开始死活不明白为什么用二分类乘积的方法就可以计算$w$在其上下文的条件概率，就是因为忽略了这里，其实$X_w$在二分类中已经被当作条件使用了，太蠢了，这点无知错误都没想明白</font></strong></p><p><img src="https://img-blog.csdnimg.cn/20200603202110178.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1ZhcmlhYmxlWA==,size_16,color_FFFFFF,t_70#pic_center" alt="img"></p><p><img src="/2022/05/24/word2vec/image-20220525011220421.png" alt="image-20220525011220421"></p><p><br></p><p><strong><font color="LightCoral" size="4">在得到了一个词在其上下文的概率之后，就可以扩展到整个词库，从而得到损失函数，采用随机梯度上升法将这个函数最大化。</font></strong></p><p><br></p><p><img src="/2022/05/24/word2vec/image-20220525012323049.png" alt="image-20220525012323049"></p><p><br></p><p><strong><font color="LightSeaGreen" size="4">$Skip-gram$</font></strong></p><p><br></p><p>讲完了$CBOW$，其实$Skip-gram$的思想一模一样，就是反过来了,之前的$CBOW$是根据上下文预测中间词，而$Skip-gram$是根据中间词预测上下文，即有了$X_w$来得到$p(context(w)|w)$，根据这个思想得到优化函数，其实就是要对$context(w)$的所有词概率进行相乘，和$CBOW$是反的，可以理解为一个是多对一，一个是一对多，但本质上都是为了让$w$和$context(w)$联系的更加紧密</p><p><br></p><p><img src="https://img-blog.csdnimg.cn/20200603202953553.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1ZhcmlhYmxlWA==,size_16,color_FFFFFF,t_70#pic_center" alt="img"></p><p><img src="/2022/05/24/word2vec/image-20220525013057616.png" alt="image-20220525013057616"></p><p><br></p><p>同样，采用随机梯度上升法将这个函数最大化。</p><hr><h2 id="Negative-Sampling"><a href="#Negative-Sampling" class="headerlink" title="Negative Sampling"></a>Negative Sampling</h2><p><img src="/2022/05/24/word2vec/image-20220525014133991.png" alt="image-20220525014133991"></p><p><br></p><p><strong><font color="LightSeaGreen" size="4">$CBOW$</font></strong></p><p><br></p><p><img src="/2022/05/24/word2vec/image-20220525014312156.png" alt="image-20220525014312156"></p><p><img src="/2022/05/24/word2vec/image-20220525014343905.png" alt="image-20220525014343905"></p><p><br></p><p><strong><font color="LightSeaGreen" size="4">$Skip-gram$</font></strong></p><p><br></p><p><img src="/2022/05/24/word2vec/image-20220525014705835.png" alt="image-20220525014705835"></p><p><img src="/2022/05/24/word2vec/image-20220525014728040.png" alt="image-20220525014728040"></p><p><br></p><p><strong><font color="LightSeaGreen" size="4">如何负采样</font></strong></p><p><br></p><p>上面个介绍了CBOW和skip-gram的负采样方法，那刚才的遗留问题，究竟怎样负采样呢？</p><p><img src="/2022/05/24/word2vec/image-20220525015241713.png" alt="image-20220525015241713"></p><p><img src="/2022/05/24/word2vec/image-20220525015246285.png" alt="image-20220525015246285"></p><p><br></p><p>参考资料：</p><ol><li><p><a href="https://www.cnblogs.com/peghoty/p/3857839.html">https://www.cnblogs.com/peghoty/p/3857839.html</a></p></li><li><p><a href="https://blog.csdn.net/VariableX/article/details/106531987">https://blog.csdn.net/VariableX/article/details/106531987</a></p></li><li><p><a href="https://www.bilibili.com/video/BV1hy4y1n7ik?p=3&amp;spm_id_from=pageDriver">https://www.bilibili.com/video/BV1hy4y1n7ik?p=3&amp;spm_id_from=pageDriver</a></p><p>explain:白板推导系列很不错</p></li></ol><hr><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><strong><font color="LightCoral" size="4">这一次下了比较大的功夫，终于对word2vec有一个比较深入的理解了，之前总是看了一知半解，不够认真，学东西的时候一定更沉下心，要不就是在浪费时间，原理明白了，但是对代码的实现还差的很远，继续加油吧。</font></strong></p>]]></content>
      
      
      <categories>
          
          <category> Paper Reading </category>
          
          <category> Natural Language Processing </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DCRNN(ICLR 2018)</title>
      <link href="/2022/05/23/DCRNN/"/>
      <url>/2022/05/23/DCRNN/</url>
      
        <content type="html"><![CDATA[<p><font color="VioletRed">paper</font>:<a href="https://arxiv.org/abs/1707.01926">https://arxiv.org/abs/1707.01926</a></p><p><font color="VioletRed">code</font>:<a href="https://github.com/liyaguang/DCRNN/">https://github.com/liyaguang/DCRNN/</a></p><h2 id="所解决的问题"><a href="#所解决的问题" class="headerlink" title="所解决的问题"></a>所解决的问题</h2><ol><li>复杂的路网空间依赖性</li><li>随着路况变化非线性变化的动态时间依赖性</li><li>长期预测本身就存在的内在的困难。</li></ol><p>这项工作使用一个<strong><font color="DarkViolet">有向图来表示交通传感器之间的成对空间相关性，该图的节点是传感器，边权重表示通过道路网络距离测量的传感器对之间的接近度。</font></strong>我们将交通流动力学建模为一个扩散过程，并提出了扩散卷积运算来捕获空间依赖性。我们进一步提出了扩散卷积递归神经网络（DCRNN），它集成了扩散卷积、序列到序列结构和定时采样技术。</p><h2 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h2><p>交通预测的目标是根据之前从道路网络上的N个相关传感器观测到的交通流量，预测未来的交通速度。我们可以将传感器网络表示为加权有向图$\mathcal G=(\mathcal V,\mathcal E,W )$，其中$\mathcal V$是一组节点$\left|\mathcal V \right|=N$，$\mathcal E$是一组边，$W \in \mathbb R^{N \times N}$是表示节点接近度的加权邻接矩阵（例如，其道路网络距离的函数）,将$\mathcal G$上观察到的交通流表示为图形信号$X \in \mathbb R^{N \times P}$，其中P是每个节点的特征数（例如速度、体积）。假设$X^{(t)}$表示在时间t观察到的图形信号，交通预测问题旨在学习一个函数$h(\cdot)$，该函数将之前的$T^{‘}$个历史图形信号映射到未来的$T$个图形信号，给定一个图G：</p><script type="math/tex; mode=display">\begin{equation}[X^{(t-T'+1)},\cdots, X^{(t)};\mathcal G] \stackrel{h(\cdot)}{\longrightarrow}[X^{(t+1)},\cdots, X^{(T)}]\end{equation}</script><h2 id="空间依赖性"><a href="#空间依赖性" class="headerlink" title="空间依赖性"></a>空间依赖性</h2><p>通过将交通流关联到扩散过程来建模空间依赖性，该过程明确捕获了交通动力学的随机性质。该扩散过程的特征是$\mathcal G$上的随机游动，restart概率为$\alpha \in [0,1]$和状态转移矩阵$D_0^{-1}W$。这里$D_0=diag(W1)$是出度对角矩阵，其中$1\in \mathbb R^{N}$是全为1的向量，<strong><font color="DarkViolet">如同马尔可夫过程一样，这个随机游走在游走了足够长的步数后能得到一个稳定的分布$\mathcal{P}\in\mathbb{R}^{N\times N}$，在这个分布中的每一行$\mathcal{P}_{i,:}\in\mathbb{R}^{N}$,表示节点$i$与其余节点的相似性。</font></strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">补充：这块内容其实在PPNP&amp;APPNP那篇里介绍过，在jk-net那篇论文里有证明说GCN获得的表征最终会与随机游走获得的稳定分布一致，但传统随机游走由于没考虑restart，使得其在表征起始节点上并不好，所以该模型加入了一个restart的概率来改进。</span><br></pre></td></tr></table></figure><p>这个稳定分布用数学公式表示为：</p><script type="math/tex; mode=display">\begin{equation}\mathcal P = \sum_{\mathcal k=0}^{\infty}\alpha(1-\alpha)^k(D_O^{-1}W)^k\end{equation}</script><p>其中$k$是扩散步数。在实践中，我们使用扩散过程的有限$k$步截断，并为每个步骤指定一个可训练的权重。<strong><font color="DarkViolet">在实际模型中，还会利用入度矩阵再求一次，以更充分地捕获双向（upstream和downstream）的信息。（注意，是有向图，所以按入度和出度划分）</font></strong></p><h2 id="扩散卷积"><a href="#扩散卷积" class="headerlink" title="扩散卷积"></a><strong>扩散卷积</strong></h2><p>$X \in \mathbb R^{N \times P}$ 和过滤器 $f_{\theta}$ 被定义为:</p><script type="math/tex; mode=display">X_{:,p\,\star \mathcal G} \,f_{\theta}=\sum_{k=0}^{K-1}(\theta_{k,1}(D_O^{-1}W)^k+ \theta_{k,2}(D_I^{-1}W^T)^k)X_{:,p} \quad for \ \mathcal p\in \left\{ 1,\cdots,P \right\}</script><p>这里的$D_O^{-1}W$和$D_I^{-1}W^T$分别是由上面的出度、入度扩散操作所得的稳定分布，式中$\theta \in \mathbb R^{K \times 2}$是滤波器的参数,上式的计算如果$\mathcal G$是稀疏矩阵，可以使用递归稀疏稠密矩阵的计算方式大大减低计算复杂度。证明可以看论文详细介绍。</p><h2 id="扩散卷积层"><a href="#扩散卷积层" class="headerlink" title="扩散卷积层"></a><strong>扩散卷积层</strong></h2><p>利用上式中定义的卷积运算，我们可以建立一个扩散卷积层，将$P$维特征映射到$Q$维输出。其中定义参数张量为$\Theta \in \mathbb R^{Q \times P \times K \times 2} = [\theta]_{q,p}$是用来进行维度转化的参数， $\Theta_{q,p,:,:} \in \mathbb R^{K \times 2}$是参数化第$p$维输入和第$q$维输出的卷积滤波器。因此，扩散卷积层为：</p><script type="math/tex; mode=display">H_{:,q}=a(\sum_{p=1}^P X_{;,q \ \star \mathcal G}\ f_{\theta_{q,p,:,:}}) \quad for \ q\in \left\{ 1, \cdots,Q \right\}</script><p>值得一提的是，文章里提到了扩散卷积层与频域GCN的关系。文中指出，ChebNet实际上是扩散卷积的一种特例。$X \in \mathbb R^{N \times P}$是输入，$H \in \mathbb R^{N \times Q}$是输出，扩散卷积层学习图结构数据的表示，我们可以使用基于随机梯度的方法对其进行训练。</p><h2 id="时间依赖性"><a href="#时间依赖性" class="headerlink" title="时间依赖性"></a><strong>时间依赖性</strong></h2><p>利用递归神经网络（RNN）来建模时间依赖性。特别是，我们使用门控循环单元（GRU），这是RNN的一种简单而强大的变体。我们将GRU中的矩阵乘法替换为扩散卷积，从而提出了扩散卷积门控循环单元（DCGRU）。</p><hr><h3 id="GRU补充知识："><a href="#GRU补充知识：" class="headerlink" title="GRU补充知识："></a>GRU补充知识：</h3><p>GRU 原论文：<a href="https://arxiv.org/pdf/1406.1078v3.pdf">https://arxiv.org/pdf/1406.1078v3.pdf</a><br>GRU不错的理解：<a href="https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be">https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be</a></p><p>GRU 背后的原理与 LSTM 非常相似，即用门控机制控制输入、记忆等信息而在当前时间步做出预测，表达式由以下给出：</p><script type="math/tex; mode=display">z=\sigma(x_tU^z + s_{t-1}W^z) \\r=\sigma(x_tU^T + S_{t-1}W^r) \\h=tanh(x_tU^T + S_{t-1}\circ W^h) \\s_t=(1-z)\circ h+ z \circ_{t-1}</script><p>GRU 有两个有两个门，即一个重置门（reset gate）和一个更新门（update gate）。从直观上来说，重置门决定了如何将新的输入信息与前面的记忆相结合，更新门定义了前面记忆保存到当前时间步的量。如果我们将重置门设置为 1，更新门设置为 0，那么我们将再次获得标准 RNN 模型。</p><h4 id="更新门"><a href="#更新门" class="headerlink" title="更新门"></a>更新门</h4><p>在时间步 t，我们首先需要使用以下公式计算更新门 $z_t$：</p><script type="math/tex; mode=display">z_t=\sigma(W^{(z)}x_t + U^{(z)}h_{t-1})</script><p>其中 $x_t$为第 $t$ 个时间步的输入向量，即输入序列 $X$的第 $t$个分量，它会经过一个线性变换（与权重矩阵 $W^{(z)}$相乘）。$h_{t-1} $保存的是前一个时间步 $t-1 $的信息，它同样也会经过一个线性变换。更新门将这两部分信息相加并投入到 $\sigma$ 激活函数中，因此将激活结果压缩到 0 到 1 之间。</p><p><strong><font color="Salmon">更新门帮助模型决定到底要将多少过去的信息传递到未来，或到底前一时间步和当前时间步的信息有多少是需要继续传递的。这一点非常强大，因为模型能决定从过去复制所有的信息以减少梯度消失的风险。</font></strong></p><h4 id="重置门"><a href="#重置门" class="headerlink" title="重置门"></a>重置门</h4><p>重置门主要决定了到底有多少过去的信息需要遗忘，我们可以使用以下表达式计算：</p><script type="math/tex; mode=display">r_t=\sigma(W^{(r)}x_t + U^{(r)}h_{t-1})</script><p><strong><font color="DarkViolet">该表达式与更新门的表达式是一样的，只不过线性变换的参数和用处不一样而已。</font></strong></p><h4 id="当前记忆内容"><a href="#当前记忆内容" class="headerlink" title="当前记忆内容"></a>当前记忆内容</h4><p>在重置门的使用中，新的记忆内容将使用重置门储存过去相关的信息，它的计算表达式为：</p><script type="math/tex; mode=display">h_t^{'}=tanh(Wx_t + r_t\circ Uh_{t-1})</script><p>输入$x_t$与上一时间步信息 $h_{t-1} $先经过一个线性变换，即分别右乘矩阵 W 和 U。计算重置门 $r_t$ 与 $Uh_{t-1}$ 的 Hadamard 乘积，即$r_t$ 与 $Uh_{t-1}$ 的对应元素乘积。<strong><font color="DarkViolet">因为前面计算的重置门是一个由 0 到 1 组成的向量，它会衡量门控开启的大小。例如某个元素对应的门控值为 0，那么它就代表这个元素的信息完全被遗忘掉。该 Hadamard 乘积将确定所要保留与遗忘的以前信息。</font></strong></p><h4 id="当前时间步的最终记忆"><a href="#当前时间步的最终记忆" class="headerlink" title="当前时间步的最终记忆"></a>当前时间步的最终记忆</h4><p>在最后一步，网络需要计算$ h_t$，该向量将保留当前单元的信息并传递到下一个单元中。在这个过程中，我们需要使用更新门，它决定了当前记忆内容和$h_{t}^{‘}$前一时间步 $h_{t-1}$ 中需要收集的信息是什么。</p><script type="math/tex; mode=display">h_t= z_t \odot h_{t-1} + (1-z_t) \odot h_t^{'}</script><p>现在我们有了<strong><font color="Salmon">当前记忆</font></strong>保留至最终记忆的信息$h_{t}^{‘}$，$z_t $与 $h_{t-1}$ 的 Hadamard 乘积表示<strong><font color="Salmon">前一时间步</font></strong>保留到最终记忆的信息</p><p><img src="/2022/05/23/DCRNN/GRU%20ALL.png" alt="GRU ALL"></p><p>门控循环单元不会随时间而清除以前的信息，它会保留相关的信息并传递到下一个单元，因此它利用全部信息而避免了梯度消失问题。</p><hr><p>回到DCRNN当中，我们将GRU中的矩阵乘法替换为扩散卷积，从而提出了扩散卷积选通递归单元（DCGRU）。公式如下：</p><script type="math/tex; mode=display">r^{(t)}=\sigma(\Theta_{r\ \star \mathcal G}[X^{(t)},H^{(t-1)}]+ b_r) \\u^{(t)}=\sigma(\Theta_{r\ \star \mathcal G}[X^{(t)},H^{(t-1)}]+ b_u) \\C^{(t)}=\tanh(\Theta_{C\ \star \mathcal G}[X^{(t)},(r^{(t)} \odot H^{(t-1)})]+ b_c) \\H^{(t)}= u^{(t)} \odot H^{(t-1)} + (1-u^{(t)}) \odot C^{(t)}</script><p>之后为了进行预测，模型在这一块设计成了$Seq2Seq$的形式。同时，为了提升$Seq2Seq$的效果，模型引入了$schedule sample$，为什么$schedule sample$能提升效果呢？原因如下：</p><p>$seq2seq$模型在训练和预测的时候实际上存在着差异，在训练过程中，是将已有的正确的序列输入进行预测，而在预测层中，则是根据上一轮生成的结果进行预测，如果上一轮结果错误，那么后续接连错误的概率就会很大。为了解决这个问题，$schedule sample$设定了一个概率$p$，使得在训练的过程中，有$p$的概率使用训练样本，有$1-p$的概率使用上一轮生成的结果进行预测。在DCRNN的训练策略中，还会随着训练的次数加深不断降低$p$，直到$p$为0，这样就使得模型能很好地适应预测阶段的模式。</p><p><strong><font color="SandyBrown">参考</font></strong>：这个作者的理解，挺不错的 <a href="https://www.ooordinary.com/post/dcrnn">https://www.ooordinary.com/post/dcrnn</a></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>第一篇时空图神经网络的文章，其实在序列这里一直是短板，没能理解是怎么序列实现的，自己又懒，一定要看一下RNN和$seq2seq$的代码实现</p><p>本篇文章主要理解的就是两个点：</p><ol><li><strong><font color="Salmon">随机游走得到稳定分布并利用扩散卷积</font></strong></li><li><strong><font color="Salmon">对时间依赖性GRU单元的理解和DCGRU的理解</font></strong></li></ol>]]></content>
      
      
      <categories>
          
          <category> Paper Reading </category>
          
          <category> Graph Neural Networks </category>
          
          <category> spatiotemporal sequences </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GNNs </tag>
            
            <tag> spatiotemporal sequences </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pro-GNN(KDD 2020)</title>
      <link href="/2022/05/21/Pro-GNN/"/>
      <url>/2022/05/21/Pro-GNN/</url>
      
        <content type="html"><![CDATA[<p><font color="VioletRed">paper</font>:<a href="https://arxiv.org/abs/2005.10203">https://arxiv.org/abs/2005.10203</a></p><p><font color="VioletRed">code</font>:</p><ol><li><a href="https://github.com/ChandlerBang/Pro-GNN/">https://github.com/ChandlerBang/Pro-GNN/</a></li><li><a href="https://github.com/DSE-MSU/DeepRobust/blob/2bcde200a5969dae32cddece66206a52c87c43e8/deeprobust/graph/defense/prognn.py">https://github.com/DSE-MSU/DeepRobust/blob/2bcde200a5969dae32cddece66206a52c87c43e8/deeprobust/graph/defense/prognn.py</a> </li></ol><p>GNN容易受到精心设计的干扰，称为对抗性攻击。对抗性攻击很容易欺骗GNN对下游任务进行预测。因此，开发鲁棒算法来防御对抗性攻击具有重要意义。防御对抗性攻击的一个自然想法是<strong><font color="DarkViolet">清除扰动图</font></strong>。很明显，真实世界的图具有一些固有的特性。例如，许多真实世界的图是<strong><font color="DarkViolet">低秩和稀疏的</font></strong>，并且两个相邻节点的特征往往相似。提出了一个通用框架Pro-GNN，它<strong><font color="DarkViolet">可以在这些属性的指导下，从扰动图中联合学习结构图和鲁棒图神经网络模型</font></strong>。</p><h2 id="探索低秩和稀疏性属性"><a href="#探索低秩和稀疏性属性" class="headerlink" title="探索低秩和稀疏性属性"></a>探索低秩和稀疏性属性</h2><p>许多现实世界的图自然是低等级和稀疏的，因为实体通常倾向于形成社区，并且只与少数邻居相连，<strong><font color="DarkViolet">对GCN的对抗性攻击往往会增加连接不同社区节点的对抗性边缘，因为这样可以更有效地降低GCN的节点分类性能</font></strong>。因此，为了从有噪声和扰动的图中恢复干净的图结构，一种可能的方法是通过强制使用具有低秩和稀疏性的<strong>新邻接矩阵S</strong>来学习接近中毒图邻接矩阵的干净邻接矩阵。给定中毒图的邻接矩阵A，我们可以将上述过程表述为结构学习问题：</p><script type="math/tex; mode=display">\begin{equation}    \mathop{\arg\min}_{S \in \mathcal S} \ \mathcal L_0 = \left\|A-S\right\|_F^2 + R(S) \quad s.t.,S=S^\mathsf{T} \end{equation}</script><p>由于对抗性攻击的目标是对图形执行<strong><font color="DarkViolet">不可见的干扰</font></strong>，因此第一项$\left|A-S\right|_F^2$确保新邻接矩阵S应接近A，由于我们假设图是无向的，新邻接矩阵应是对称的，即$S=S^\mathsf{T}$, R(S)表示S上的约束，以增强低秩和稀疏性的性质，那R(S)该如何定义呢？根据一些研究，<strong><font color="DarkViolet">最小化矩阵的1范数和核范数可以分别强制矩阵稀疏和低秩</font></strong>。因此上式就可变为</p><script type="math/tex; mode=display">\begin{equation}    \mathop{\arg\min}_{S \in \mathcal S} \ \mathcal L_0 = \left\|A-S\right\|_F^2 + \alpha \left\|S\right\|_1 +\beta \left\|S\right\|_* \quad s.t.,S=S^\mathsf{T} \end{equation}</script><p>其中，$\alpha$和$\beta$是预定义的参数，<strong><font color="DarkViolet">分别控制稀疏性和低秩属性的贡献</font></strong>。最大限度地减少核范数$\left|S\right|_*$的一个重要好处是我们可以减少每一个奇异值，从而减轻对抗性攻击扩大奇异值的影响。</p><h2 id="探索特征平滑度"><a href="#探索特征平滑度" class="headerlink" title="探索特征平滑度"></a>探索特征平滑度</h2><p>很明显，图中的连接节点可能具有相似的特征。事实上，这种观察是在许多领域的图上进行的。例如，社交图中的两个连接用户可能共享相似的属性，网页图中的两个链接网页往往具有相似的内容，引文网络中的两篇连接论文通常具有相似的主题。同时，最近有证据表明，<strong><font color="DarkViolet">对图的对抗性攻击倾向于连接具有不同特征的节点</font></strong>。因此，我们的目标是确保所学习到的图中的特征平滑。特征平滑度可通过以下术语$\mathcal L_s$获得</p><script type="math/tex; mode=display">\begin{equation}     \mathcal L_s = \frac{1}{2}\sum_{i,j=1}^{N}S_{ij}(x_i-x_j)^2\end{equation}</script><p>其中S是新的邻接矩阵，$S_{ij}$表示学习图中$v_i$和$v_j$的连接，以及$(x_i-x_j)^2$测量$v_i$和$v_j$之间的特性差异。$\mathcal L_s$可以重写为：</p><script type="math/tex; mode=display">\begin{equation}     \mathcal L_s = tr(X^\mathsf{T}LX)\end{equation}</script><p>其中$L=D− S$是$S$图的Laplacian矩阵，$D$是$S$的对角矩阵。在这项工作中，我们使用归一化Laplacian矩阵$\hat L=D^{-1/2}LD^{-1/2}$而不是L，以使特征平滑度独立于图形节点的度数，所以此时的$\mathcal L_s$就变成了如下的形式：</p><script type="math/tex; mode=display">\begin{equation}     \mathcal L_s = tr(X^\mathsf{T}\hat LX) = \frac{1}{2}\sum_{i,j=1}^{N}S_{ij}(\frac{x_i}{\sqrt{d_i}} - \frac{x_j}{\sqrt{d_j}})^2\end{equation}</script><p>其中$d_i$表示学习图中$v_i$的阶数，在学习到的图中，如果$v_i$和$v_j$是连接的(即$S_{ij}\neq0$)，即特征差异$(x_i-x_j)^2$应较小。换言之，如果两个连接的节点之间的特征非常不同，$\mathcal L_s$非常大。因此，$\mathcal L_s$越小，图$\mathcal S$上的特征X越平滑。因此，为了实现所学习图中的特征平滑，我们应该最小化$\mathcal L_s$。因此，我们可以将特征平滑度项添加到的目标函数中，以惩罚相邻节点之间特征的快速变化，如下所示</p><script type="math/tex; mode=display">\begin{equation}    \mathop{\arg\min}_{S \in \mathcal S} \ \mathcal L = \mathcal L_0 + \lambda\mathcal L_s =\mathcal L_0 + \lambda tr(X^\mathsf{T}LX) \quad s.t.,S=S^\mathsf{T} \end{equation}</script><p>其中$\lambda$是一个预定义参数，用于控制特征平滑度的贡献。</p><h2 id="Pro-GNN的目标函数"><a href="#Pro-GNN的目标函数" class="headerlink" title="Pro-GNN的目标函数"></a>Pro-GNN的目标函数</h2><p>首先通过上面的式子从中毒图中学习一个图，然后用所学习的图训练GNN模型。然而，在这种两阶段策略下，对于给定任务的GNN模型，学习的图可能是次优的。因此，我们提出了一种更好的策略来联合学习特定下游任务的图结构和GNN模型。我们的经验表明，<strong><font color="DarkViolet">联合学习GNN模型和邻接矩阵优于两阶段</font></strong>。Pro-GNN的最终目标函数如下所示</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">简单说就是两阶段是先对图进行净化，再用这个图去训练GNN，而现在联合学习，一边训练一边优化</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">\begin{equation}    \mathop{\arg\min}_{S \in \mathcal S,\theta} \ \mathcal L = \mathcal L_0 + \lambda\mathcal L_s + \gamma\mathcal L_{GNN}  = \left\|A-S\right\|_F^2 + \alpha \left\|S\right\|_1 +\beta \left\|S\right\|_* + \lambda tr(X^\mathsf{T}\hat LX) + \gamma\mathcal L_{GNN}(\theta,\mathcal S,X,\mathcal Y_L)  \quad s.t.,S=S^\mathsf{T} \end{equation}</script><p>该公式的另一个好处是，来自$\mathcal L_{GNN}$还可以指导图形学习过程，以抵御对抗性攻击，因为图形对抗性攻击的目标是最大化$\mathcal L_{GNN}$,所以我们在防御的时候要将这个值变小.</p><p>下面是Pro-GNN的整体框架图，非常的好理解</p><p><img src="/2022/05/21/Pro-GNN/Pro-GNN.png" alt></p><h2 id="优化方法"><a href="#优化方法" class="headerlink" title="优化方法"></a>优化方法</h2><p>联合优化等式上述等式中的$\theta$和$\mathcal S$是一项挑战。对$\mathcal S$的限制进一步加剧了这一困难。因此，在这项工作中，我们使用<strong><font color="DarkViolet">交替优化模式来迭代更新θ和S</font></strong>。</p><h3 id="更新-theta"><a href="#更新-theta" class="headerlink" title="更新$\theta$"></a>更新$\theta$</h3><p>为了更新θ，固定S并删除与θ无关的项，然后目标函数减少为：</p><script type="math/tex; mode=display">\min_\theta \mathcal L_{GNN}(\theta,S,X,\mathcal Y_l) = \sum_{\mathcal u \in\mathcal V_L} \ell(f_\theta(X,S)_u,\mathcal y_u)</script><p>这是一个典型的GNN优化问题，我们可以通过随机梯度下降来学习$\theta$。</p><h3 id="更新-mathcal-S"><a href="#更新-mathcal-S" class="headerlink" title="更新$\mathcal S$"></a>更新$\mathcal S$</h3><p>类似地，为了更新$\mathcal S$，我们固定$\theta$并得出</p><script type="math/tex; mode=display">\min_S \mathcal L(S,A) + \alpha \left\|S\right\|_1 +\beta \left\|S\right\|_* \quad s.t.,S=S^\mathsf{T}</script><p>其中第一项为</p><script type="math/tex; mode=display">\mathcal L(S,A)= \left\|A-S\right\|_F^2 +  \gamma\mathcal L_{GNN}(\theta,\mathcal S,X,\mathcal Y_L) + \lambda tr(X^\mathsf{T}\hat LX)</script><p>请注意，<strong><font color="DarkViolet">ℓ1范数和核范数是不可微的</font></strong>。对于只有一个非微分正则化子R(S)的优化问题，我们可以使用前向-后向分裂方法（<strong><font color="FireBrick">具体实现请看论文和代码</font></strong>）。想法是交替使用梯度下降步骤和近似步骤</p><h1 id="总结："><a href="#总结：" class="headerlink" title="总结："></a><strong>总结：</strong></h1><p>本笔记重在理解两个部分</p><ol><li><strong><font color="Salmon">对中毒图的净化方法，包括控制低秩稀疏的$\ell_1$范数和核范数，还有特征平滑的方法</font></strong></li><li><strong><font color="Salmon">交替优化代替两阶段优化的优化方法</font></strong></li></ol>]]></content>
      
      
      <categories>
          
          <category> Paper Reading </category>
          
          <category> Graph Neural Networks </category>
          
          <category> Attacks and defends </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GNNs </tag>
            
            <tag> Attacks and defends </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>

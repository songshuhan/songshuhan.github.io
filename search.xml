<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>DCRNN(ICLR 2018)</title>
      <link href="/2022/05/23/DCRNN/"/>
      <url>/2022/05/23/DCRNN/</url>
      
        <content type="html"><![CDATA[<p><font color="VioletRed">paper</font>:<a href="https://arxiv.org/abs/1707.01926">https://arxiv.org/abs/1707.01926</a></p><p><font color="VioletRed">code</font>:<a href="https://github.com/liyaguang/DCRNN/">https://github.com/liyaguang/DCRNN/</a></p><h2 id="所解决的问题"><a href="#所解决的问题" class="headerlink" title="所解决的问题"></a>所解决的问题</h2><ol><li>复杂的路网空间依赖性</li><li>随着路况变化非线性变化的动态时间依赖性</li><li>长期预测本身就存在的内在的困难。</li></ol><p>这项工作使用一个<strong><font color="DarkViolet">有向图来表示交通传感器之间的成对空间相关性，该图的节点是传感器，边权重表示通过道路网络距离测量的传感器对之间的接近度。</font></strong>我们将交通流动力学建模为一个扩散过程，并提出了扩散卷积运算来捕获空间依赖性。我们进一步提出了扩散卷积递归神经网络（DCRNN），它集成了扩散卷积、序列到序列结构和定时采样技术。</p><h2 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h2><p>交通预测的目标是根据之前从道路网络上的N个相关传感器观测到的交通流量，预测未来的交通速度。我们可以将传感器网络表示为加权有向图$\mathcal G=(\mathcal V,\mathcal E,W )$，其中$\mathcal V$是一组节点$\left|\mathcal V \right|=N$，$\mathcal E$是一组边，$W \in \mathbb R^{N \times N}$是表示节点接近度的加权邻接矩阵（例如，其道路网络距离的函数）,将$\mathcal G$上观察到的交通流表示为图形信号$X \in \mathbb R^{N \times P}$，其中P是每个节点的特征数（例如速度、体积）。假设$X^{(t)}$表示在时间t观察到的图形信号，交通预测问题旨在学习一个函数$h(\cdot)$，该函数将之前的$T^{‘}$个历史图形信号映射到未来的$T$个图形信号，给定一个图G：</p><script type="math/tex; mode=display">\begin{equation}[X^{(t-T'+1)},\cdots, X^{(t)};\mathcal G] \stackrel{h(\cdot)}{\longrightarrow}[X^{(t+1)},\cdots, X^{(T)}]\end{equation}</script><h2 id="空间依赖性"><a href="#空间依赖性" class="headerlink" title="空间依赖性"></a>空间依赖性</h2><p>通过将交通流关联到扩散过程来建模空间依赖性，该过程明确捕获了交通动力学的随机性质。该扩散过程的特征是$\mathcal G$上的随机游动，restart概率为$\alpha \in [0,1]$和状态转移矩阵$D_0^{-1}W$。这里$D_0=diag(W1)$是出度对角矩阵，其中$1\in \mathbb R^{N}$是全为1的向量，<strong><font color="DarkViolet">如同马尔可夫过程一样，这个随机游走在游走了足够长的步数后能得到一个稳定的分布$\mathcal{P}\in\mathbb{R}^{N\times N}$，在这个分布中的每一行$\mathcal{P}_{i,:}\in\mathbb{R}^{N}$,表示节点$i$与其余节点的相似性。</font></strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">补充：这块内容其实在PPNP&amp;APPNP那篇里介绍过，在jk-net那篇论文里有证明说GCN获得的表征最终会与随机游走获得的稳定分布一致，但传统随机游走由于没考虑restart，使得其在表征起始节点上并不好，所以该模型加入了一个restart的概率来改进。</span><br></pre></td></tr></table></figure><p>这个稳定分布用数学公式表示为：</p><script type="math/tex; mode=display">\begin{equation}\mathcal P = \sum_{\mathcal k=0}^{\infty}\alpha(1-\alpha)^k(D_O^{-1}W)^k\end{equation}</script><p>其中$k$是扩散步数。在实践中，我们使用扩散过程的有限$k$步截断，并为每个步骤指定一个可训练的权重。<strong><font color="DarkViolet">在实际模型中，还会利用入度矩阵再求一次，以更充分地捕获双向（upstream和downstream）的信息。（注意，是有向图，所以按入度和出度划分）</font></strong></p><h2 id="扩散卷积"><a href="#扩散卷积" class="headerlink" title="扩散卷积"></a><strong>扩散卷积</strong></h2><p>$X \in \mathbb R^{N \times P}$ 和过滤器 $f_{\theta}$ 被定义为:</p><script type="math/tex; mode=display">X_{:,p\,\star \mathcal G} \,f_{\theta}=\sum_{k=0}^{K-1}(\theta_{k,1}(D_O^{-1}W)^k+ \theta_{k,2}(D_I^{-1}W^T)^k)X_{:,p} \quad for \ \mathcal p\in \left\{ 1,\cdots,P \right\}</script><p>这里的$D_O^{-1}W$和$D_I^{-1}W^T$分别是由上面的出度、入度扩散操作所得的稳定分布，式中$\theta \in \mathbb R^{K \times 2}$是滤波器的参数,上式的计算如果$\mathcal G$是稀疏矩阵，可以使用递归稀疏稠密矩阵的计算方式大大减低计算复杂度。证明可以看论文详细介绍。</p><h2 id="扩散卷积层"><a href="#扩散卷积层" class="headerlink" title="扩散卷积层"></a><strong>扩散卷积层</strong></h2><p>利用上式中定义的卷积运算，我们可以建立一个扩散卷积层，将$P$维特征映射到$Q$维输出。其中定义参数张量为$\Theta \in \mathbb R^{Q \times P \times K \times 2} = [\theta]_{q,p}$是用来进行维度转化的参数， $\Theta_{q,p,:,:} \in \mathbb R^{K \times 2}$是参数化第$p$维输入和第$q$维输出的卷积滤波器。因此，扩散卷积层为：</p><script type="math/tex; mode=display">H_{:,q}=a(\sum_{p=1}^P X_{;,q \ \star \mathcal G}\ f_{\theta_{q,p,:,:}}) \quad for \ q\in \left\{ 1, \cdots,Q \right\}</script><p>值得一提的是，文章里提到了扩散卷积层与频域GCN的关系。文中指出，ChebNet实际上是扩散卷积的一种特例。$X \in \mathbb R^{N \times P}$是输入，$H \in \mathbb R^{N \times Q}$是输出，扩散卷积层学习图结构数据的表示，我们可以使用基于随机梯度的方法对其进行训练。</p><h2 id="时间依赖性"><a href="#时间依赖性" class="headerlink" title="时间依赖性"></a><strong>时间依赖性</strong></h2><p>利用递归神经网络（RNN）来建模时间依赖性。特别是，我们使用门控循环单元（GRU），这是RNN的一种简单而强大的变体。我们将GRU中的矩阵乘法替换为扩散卷积，从而提出了扩散卷积门控循环单元（DCGRU）。</p><hr><h3 id="GRU补充知识："><a href="#GRU补充知识：" class="headerlink" title="GRU补充知识："></a>GRU补充知识：</h3><p>GRU 原论文：<a href="https://arxiv.org/pdf/1406.1078v3.pdf">https://arxiv.org/pdf/1406.1078v3.pdf</a><br>GRU不错的理解：<a href="https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be">https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be</a></p><p>GRU 背后的原理与 LSTM 非常相似，即用门控机制控制输入、记忆等信息而在当前时间步做出预测，表达式由以下给出：</p><script type="math/tex; mode=display">z=\sigma(x_tU^z + s_{t-1}W^z) \\r=\sigma(x_tU^T + S_{t-1}W^r) \\h=tanh(x_tU^T + S_{t-1}\circ W^h) \\s_t=(1-z)\circ h+ z \circ_{t-1}</script><p>GRU 有两个有两个门，即一个重置门（reset gate）和一个更新门（update gate）。从直观上来说，重置门决定了如何将新的输入信息与前面的记忆相结合，更新门定义了前面记忆保存到当前时间步的量。如果我们将重置门设置为 1，更新门设置为 0，那么我们将再次获得标准 RNN 模型。</p><h4 id="更新门"><a href="#更新门" class="headerlink" title="更新门"></a>更新门</h4><p>在时间步 t，我们首先需要使用以下公式计算更新门 $z_t$：</p><script type="math/tex; mode=display">z_t=\sigma(W^{(z)}x_t + U^{(z)}h_{t-1})</script><p>其中 $x_t$为第 $t$ 个时间步的输入向量，即输入序列 $X$的第 $t$个分量，它会经过一个线性变换（与权重矩阵 $W^{(z)}$相乘）。$h_{t-1} $保存的是前一个时间步 $t-1 $的信息，它同样也会经过一个线性变换。更新门将这两部分信息相加并投入到 $\sigma$ 激活函数中，因此将激活结果压缩到 0 到 1 之间。</p><p><strong><font color="Salmon">更新门帮助模型决定到底要将多少过去的信息传递到未来，或到底前一时间步和当前时间步的信息有多少是需要继续传递的。这一点非常强大，因为模型能决定从过去复制所有的信息以减少梯度消失的风险。</font></strong></p><h4 id="重置门"><a href="#重置门" class="headerlink" title="重置门"></a>重置门</h4><p>重置门主要决定了到底有多少过去的信息需要遗忘，我们可以使用以下表达式计算：</p><script type="math/tex; mode=display">r_t=\sigma(W^{(r)}x_t + U^{(r)}h_{t-1})</script><p><strong><font color="DarkViolet">该表达式与更新门的表达式是一样的，只不过线性变换的参数和用处不一样而已。</font></strong></p><h4 id="当前记忆内容"><a href="#当前记忆内容" class="headerlink" title="当前记忆内容"></a>当前记忆内容</h4><p>在重置门的使用中，新的记忆内容将使用重置门储存过去相关的信息，它的计算表达式为：</p><script type="math/tex; mode=display">h_t^{'}=tanh(Wx_t + r_t\circ Uh_{t-1})</script><p>输入$x_t$与上一时间步信息 $h_{t-1} $先经过一个线性变换，即分别右乘矩阵 W 和 U。计算重置门 $r_t$ 与 $Uh_{t-1}$ 的 Hadamard 乘积，即$r_t$ 与 $Uh_{t-1}$ 的对应元素乘积。<strong><font color="DarkViolet">因为前面计算的重置门是一个由 0 到 1 组成的向量，它会衡量门控开启的大小。例如某个元素对应的门控值为 0，那么它就代表这个元素的信息完全被遗忘掉。该 Hadamard 乘积将确定所要保留与遗忘的以前信息。</font></strong></p><h4 id="当前时间步的最终记忆"><a href="#当前时间步的最终记忆" class="headerlink" title="当前时间步的最终记忆"></a>当前时间步的最终记忆</h4><p>在最后一步，网络需要计算$ h_t$，该向量将保留当前单元的信息并传递到下一个单元中。在这个过程中，我们需要使用更新门，它决定了当前记忆内容和$h_{t}^{‘}$前一时间步 $h_{t-1}$ 中需要收集的信息是什么。</p><script type="math/tex; mode=display">h_t= z_t \odot h_{t-1} + (1-z_t) \odot h_t^{'}</script><p>现在我们有了<strong><font color="Salmon">当前记忆</font></strong>保留至最终记忆的信息$h_{t}^{‘}$，$z_t $与 $h_{t-1}$ 的 Hadamard 乘积表示<strong><font color="Salmon">前一时间步</font></strong>保留到最终记忆的信息</p><p><img src="/2022/05/23/DCRNN/GRU%20ALL.png" alt="GRU ALL"></p><p>门控循环单元不会随时间而清除以前的信息，它会保留相关的信息并传递到下一个单元，因此它利用全部信息而避免了梯度消失问题。</p><hr><p>回到DCRNN当中，我们将GRU中的矩阵乘法替换为扩散卷积，从而提出了扩散卷积选通递归单元（DCGRU）。公式如下：</p><script type="math/tex; mode=display">r^{(t)}=\sigma(\Theta_{r\ \star \mathcal G}[X^{(t)},H^{(t-1)}]+ b_r) \\u^{(t)}=\sigma(\Theta_{r\ \star \mathcal G}[X^{(t)},H^{(t-1)}]+ b_u) \\C^{(t)}=\tanh(\Theta_{C\ \star \mathcal G}[X^{(t)},(r^{(t)} \odot H^{(t-1)})]+ b_c) \\H^{(t)}= u^{(t)} \odot H^{(t-1)} + (1-u^{(t)}) \odot C^{(t)}</script><p>之后为了进行预测，模型在这一块设计成了$Seq2Seq$的形式。同时，为了提升$Seq2Seq$的效果，模型引入了$schedule sample$，为什么$schedule sample$能提升效果呢？原因如下：</p><p>$seq2seq$模型在训练和预测的时候实际上存在着差异，在训练过程中，是将已有的正确的序列输入进行预测，而在预测层中，则是根据上一轮生成的结果进行预测，如果上一轮结果错误，那么后续接连错误的概率就会很大。为了解决这个问题，$schedule sample$设定了一个概率$p$，使得在训练的过程中，有$p$的概率使用训练样本，有$1-p$的概率使用上一轮生成的结果进行预测。在DCRNN的训练策略中，还会随着训练的次数加深不断降低$p$，直到$p$为0，这样就使得模型能很好地适应预测阶段的模式。</p><p><strong><font color="SandyBrown">参考</font></strong>：这个作者的理解，挺不错的 <a href="https://www.ooordinary.com/post/dcrnn">https://www.ooordinary.com/post/dcrnn</a></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>第一篇时空图神经网络的文章，其实在序列这里一直是短板，没能理解是怎么序列实现的，自己又懒，一定要看一下RNN和$seq2seq$的代码实现</p><p>本篇文章主要理解的就是两个点：</p><ol><li><strong><font color="Salmon">随机游走得到稳定分布并利用扩散卷积</font></strong></li><li><strong><font color="Salmon">对时间依赖性GRU单元的理解和DCGRU的理解</font></strong></li></ol>]]></content>
      
      
      <categories>
          
          <category> Paper Reading </category>
          
          <category> Graph Neural Networks </category>
          
          <category> spatiotemporal sequences </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GNNs </tag>
            
            <tag> spatiotemporal sequences </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pro-GNN(KDD 2020)</title>
      <link href="/2022/05/21/Pro-GNN/"/>
      <url>/2022/05/21/Pro-GNN/</url>
      
        <content type="html"><![CDATA[<p><font color="VioletRed">paper</font>:<a href="https://arxiv.org/abs/2005.10203">https://arxiv.org/abs/2005.10203</a></p><p><font color="VioletRed">code</font>:</p><ol><li><a href="https://github.com/ChandlerBang/Pro-GNN/">https://github.com/ChandlerBang/Pro-GNN/</a></li><li><a href="https://github.com/DSE-MSU/DeepRobust/blob/2bcde200a5969dae32cddece66206a52c87c43e8/deeprobust/graph/defense/prognn.py">https://github.com/DSE-MSU/DeepRobust/blob/2bcde200a5969dae32cddece66206a52c87c43e8/deeprobust/graph/defense/prognn.py</a> </li></ol><p>GNN容易受到精心设计的干扰，称为对抗性攻击。对抗性攻击很容易欺骗GNN对下游任务进行预测。因此，开发鲁棒算法来防御对抗性攻击具有重要意义。防御对抗性攻击的一个自然想法是<strong><font color="DarkViolet">清除扰动图</font></strong>。很明显，真实世界的图具有一些固有的特性。例如，许多真实世界的图是<strong><font color="DarkViolet">低秩和稀疏的</font></strong>，并且两个相邻节点的特征往往相似。提出了一个通用框架Pro-GNN，它<strong><font color="DarkViolet">可以在这些属性的指导下，从扰动图中联合学习结构图和鲁棒图神经网络模型</font></strong>。</p><h2 id="探索低秩和稀疏性属性"><a href="#探索低秩和稀疏性属性" class="headerlink" title="探索低秩和稀疏性属性"></a>探索低秩和稀疏性属性</h2><p>许多现实世界的图自然是低等级和稀疏的，因为实体通常倾向于形成社区，并且只与少数邻居相连，<strong><font color="DarkViolet">对GCN的对抗性攻击往往会增加连接不同社区节点的对抗性边缘，因为这样可以更有效地降低GCN的节点分类性能</font></strong>。因此，为了从有噪声和扰动的图中恢复干净的图结构，一种可能的方法是通过强制使用具有低秩和稀疏性的<strong>新邻接矩阵S</strong>来学习接近中毒图邻接矩阵的干净邻接矩阵。给定中毒图的邻接矩阵A，我们可以将上述过程表述为结构学习问题：</p><script type="math/tex; mode=display">\begin{equation}    \mathop{\arg\min}_{S \in \mathcal S} \ \mathcal L_0 = \left\|A-S\right\|_F^2 + R(S) \quad s.t.,S=S^\mathsf{T} \end{equation}</script><p>由于对抗性攻击的目标是对图形执行<strong><font color="DarkViolet">不可见的干扰</font></strong>，因此第一项$\left|A-S\right|_F^2$确保新邻接矩阵S应接近A，由于我们假设图是无向的，新邻接矩阵应是对称的，即$S=S^\mathsf{T}$, R(S)表示S上的约束，以增强低秩和稀疏性的性质，那R(S)该如何定义呢？根据一些研究，<strong><font color="DarkViolet">最小化矩阵的1范数和核范数可以分别强制矩阵稀疏和低秩</font></strong>。因此上式就可变为</p><script type="math/tex; mode=display">\begin{equation}    \mathop{\arg\min}_{S \in \mathcal S} \ \mathcal L_0 = \left\|A-S\right\|_F^2 + \alpha \left\|S\right\|_1 +\beta \left\|S\right\|_* \quad s.t.,S=S^\mathsf{T} \end{equation}</script><p>其中，$\alpha$和$\beta$是预定义的参数，<strong><font color="DarkViolet">分别控制稀疏性和低秩属性的贡献</font></strong>。最大限度地减少核范数$\left|S\right|_*$的一个重要好处是我们可以减少每一个奇异值，从而减轻对抗性攻击扩大奇异值的影响。</p><h2 id="探索特征平滑度"><a href="#探索特征平滑度" class="headerlink" title="探索特征平滑度"></a>探索特征平滑度</h2><p>很明显，图中的连接节点可能具有相似的特征。事实上，这种观察是在许多领域的图上进行的。例如，社交图中的两个连接用户可能共享相似的属性，网页图中的两个链接网页往往具有相似的内容，引文网络中的两篇连接论文通常具有相似的主题。同时，最近有证据表明，<strong><font color="DarkViolet">对图的对抗性攻击倾向于连接具有不同特征的节点</font></strong>。因此，我们的目标是确保所学习到的图中的特征平滑。特征平滑度可通过以下术语$\mathcal L_s$获得</p><script type="math/tex; mode=display">\begin{equation}     \mathcal L_s = \frac{1}{2}\sum_{i,j=1}^{N}S_{ij}(x_i-x_j)^2\end{equation}</script><p>其中S是新的邻接矩阵，$S_{ij}$表示学习图中$v_i$和$v_j$的连接，以及$(x_i-x_j)^2$测量$v_i$和$v_j$之间的特性差异。$\mathcal L_s$可以重写为：</p><script type="math/tex; mode=display">\begin{equation}     \mathcal L_s = tr(X^\mathsf{T}LX)\end{equation}</script><p>其中$L=D− S$是$S$图的Laplacian矩阵，$D$是$S$的对角矩阵。在这项工作中，我们使用归一化Laplacian矩阵$\hat L=D^{-1/2}LD^{-1/2}$而不是L，以使特征平滑度独立于图形节点的度数，所以此时的$\mathcal L_s$就变成了如下的形式：</p><script type="math/tex; mode=display">\begin{equation}     \mathcal L_s = tr(X^\mathsf{T}\hat LX) = \frac{1}{2}\sum_{i,j=1}^{N}S_{ij}(\frac{x_i}{\sqrt{d_i}} - \frac{x_j}{\sqrt{d_j}})^2\end{equation}</script><p>其中$d_i$表示学习图中$v_i$的阶数，在学习到的图中，如果$v_i$和$v_j$是连接的(即$S_{ij}\neq0$)，即特征差异$(x_i-x_j)^2$应较小。换言之，如果两个连接的节点之间的特征非常不同，$\mathcal L_s$非常大。因此，$\mathcal L_s$越小，图$\mathcal S$上的特征X越平滑。因此，为了实现所学习图中的特征平滑，我们应该最小化$\mathcal L_s$。因此，我们可以将特征平滑度项添加到的目标函数中，以惩罚相邻节点之间特征的快速变化，如下所示</p><script type="math/tex; mode=display">\begin{equation}    \mathop{\arg\min}_{S \in \mathcal S} \ \mathcal L = \mathcal L_0 + \lambda\mathcal L_s =\mathcal L_0 + \lambda tr(X^\mathsf{T}LX) \quad s.t.,S=S^\mathsf{T} \end{equation}</script><p>其中$\lambda$是一个预定义参数，用于控制特征平滑度的贡献。</p><h2 id="Pro-GNN的目标函数"><a href="#Pro-GNN的目标函数" class="headerlink" title="Pro-GNN的目标函数"></a>Pro-GNN的目标函数</h2><p>首先通过上面的式子从中毒图中学习一个图，然后用所学习的图训练GNN模型。然而，在这种两阶段策略下，对于给定任务的GNN模型，学习的图可能是次优的。因此，我们提出了一种更好的策略来联合学习特定下游任务的图结构和GNN模型。我们的经验表明，<strong><font color="DarkViolet">联合学习GNN模型和邻接矩阵优于两阶段</font></strong>。Pro-GNN的最终目标函数如下所示</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">简单说就是两阶段是先对图进行净化，再用这个图去训练GNN，而现在联合学习，一边训练一边优化</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">\begin{equation}    \mathop{\arg\min}_{S \in \mathcal S,\theta} \ \mathcal L = \mathcal L_0 + \lambda\mathcal L_s + \gamma\mathcal L_{GNN}  = \left\|A-S\right\|_F^2 + \alpha \left\|S\right\|_1 +\beta \left\|S\right\|_* + \lambda tr(X^\mathsf{T}\hat LX) + \gamma\mathcal L_{GNN}(\theta,\mathcal S,X,\mathcal Y_L)  \quad s.t.,S=S^\mathsf{T} \end{equation}</script><p>该公式的另一个好处是，来自$\mathcal L_{GNN}$还可以指导图形学习过程，以抵御对抗性攻击，因为图形对抗性攻击的目标是最大化$\mathcal L_{GNN}$,所以我们在防御的时候要将这个值变小.</p><p>下面是Pro-GNN的整体框架图，非常的好理解</p><p><img src="/2022/05/21/Pro-GNN/Pro-GNN.png" alt></p><h2 id="优化方法"><a href="#优化方法" class="headerlink" title="优化方法"></a>优化方法</h2><p>联合优化等式上述等式中的$\theta$和$\mathcal S$是一项挑战。对$\mathcal S$的限制进一步加剧了这一困难。因此，在这项工作中，我们使用<strong><font color="DarkViolet">交替优化模式来迭代更新θ和S</font></strong>。</p><h3 id="更新-theta"><a href="#更新-theta" class="headerlink" title="更新$\theta$"></a>更新$\theta$</h3><p>为了更新θ，固定S并删除与θ无关的项，然后目标函数减少为：</p><script type="math/tex; mode=display">\min_\theta \mathcal L_{GNN}(\theta,S,X,\mathcal Y_l) = \sum_{\mathcal u \in\mathcal V_L} \ell(f_\theta(X,S)_u,\mathcal y_u)</script><p>这是一个典型的GNN优化问题，我们可以通过随机梯度下降来学习$\theta$。</p><h3 id="更新-mathcal-S"><a href="#更新-mathcal-S" class="headerlink" title="更新$\mathcal S$"></a>更新$\mathcal S$</h3><p>类似地，为了更新$\mathcal S$，我们固定$\theta$并得出</p><script type="math/tex; mode=display">\min_S \mathcal L(S,A) + \alpha \left\|S\right\|_1 +\beta \left\|S\right\|_* \quad s.t.,S=S^\mathsf{T}</script><p>其中第一项为</p><script type="math/tex; mode=display">\mathcal L(S,A)= \left\|A-S\right\|_F^2 +  \gamma\mathcal L_{GNN}(\theta,\mathcal S,X,\mathcal Y_L) + \lambda tr(X^\mathsf{T}\hat LX)</script><p>请注意，<strong><font color="DarkViolet">ℓ1范数和核范数是不可微的</font></strong>。对于只有一个非微分正则化子R(S)的优化问题，我们可以使用前向-后向分裂方法（<strong><font color="FireBrick">具体实现请看论文和代码</font></strong>）。想法是交替使用梯度下降步骤和近似步骤</p><h1 id="总结："><a href="#总结：" class="headerlink" title="总结："></a><strong>总结：</strong></h1><p>本笔记重在理解两个部分</p><ol><li><strong><font color="Salmon">对中毒图的净化方法，包括控制低秩稀疏的$\ell_1$范数和核范数，还有特征平滑的方法</font></strong></li><li><strong><font color="Salmon">交替优化代替两阶段优化的优化方法</font></strong></li></ol>]]></content>
      
      
      <categories>
          
          <category> Paper Reading </category>
          
          <category> Graph Neural Networks </category>
          
          <category> Attacks and defends </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GNNs </tag>
            
            <tag> Attacks and defends </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>

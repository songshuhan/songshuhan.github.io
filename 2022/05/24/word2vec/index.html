<!DOCTYPE html>
<html lang=en>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5" />
    <meta name="description" content="paper:  https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1301.3781explain: word2vec的奠基性论文之一，由Google的Tomas Mikolov提出。该论文提出了CBOW和Skip-gram两种word2vec模型结构。 https:&#x2F;&#x2F;proceedings.neurips.cc&#x2F;paper&#x2F;2013&#x2F;hash&#x2F;9aa42b31882ec039965f3c4923ce">
<meta property="og:type" content="article">
<meta property="og:title" content="(Google 2013) Word2vec">
<meta property="og:url" content="http://example.com/2022/05/24/word2vec/index.html">
<meta property="og:site_name" content="SHUHAN&#39;S NOTE">
<meta property="og:description" content="paper:  https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1301.3781explain: word2vec的奠基性论文之一，由Google的Tomas Mikolov提出。该论文提出了CBOW和Skip-gram两种word2vec模型结构。 https:&#x2F;&#x2F;proceedings.neurips.cc&#x2F;paper&#x2F;2013&#x2F;hash&#x2F;9aa42b31882ec039965f3c4923ce">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2022/05/24/word2vec/image-20220524213551921.png">
<meta property="og:image" content="http://example.com/2022/05/24/word2vec/image-20220524233941683.png">
<meta property="og:image" content="http://example.com/2022/05/24/word2vec/20200831235730940.png">
<meta property="og:image" content="http://example.com/2022/05/24/word2vec/image-20220525010308984.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200603202110178.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1ZhcmlhYmxlWA==,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="http://example.com/2022/05/24/word2vec/image-20220525011220421.png">
<meta property="og:image" content="http://example.com/2022/05/24/word2vec/image-20220525012323049.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200603202953553.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1ZhcmlhYmxlWA==,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="http://example.com/2022/05/24/word2vec/image-20220525013057616.png">
<meta property="og:image" content="http://example.com/2022/05/24/word2vec/image-20220525014133991.png">
<meta property="og:image" content="http://example.com/2022/05/24/word2vec/image-20220525014312156.png">
<meta property="og:image" content="http://example.com/2022/05/24/word2vec/image-20220525014343905.png">
<meta property="og:image" content="http://example.com/2022/05/24/word2vec/image-20220525014705835.png">
<meta property="og:image" content="http://example.com/2022/05/24/word2vec/image-20220525014728040.png">
<meta property="og:image" content="http://example.com/2022/05/24/word2vec/image-20220525015241713.png">
<meta property="og:image" content="http://example.com/2022/05/24/word2vec/image-20220525015246285.png">
<meta property="article:published_time" content="2022-05-24T10:14:07.000Z">
<meta property="article:modified_time" content="2022-07-18T03:13:30.328Z">
<meta property="article:author" content="Shuhan Song">
<meta property="article:tag" content="NLP">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2022/05/24/word2vec/image-20220524213551921.png">
    
    
      
        
          <link rel="shortcut icon" href="/images/favicon.ico">
        
      
      
        
          <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
        
      
      
        
          <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
        
      
    
    <!-- title -->
    <title>(Google 2013) Word2vec</title>
    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
    <!-- rss -->
    
    
	<!-- mathjax -->
	
		<script type="text/x-mathjax-config">
		  MathJax.Hub.Config({
			tex2jax: {
			  skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
			  inlineMath: [['$','$']]
			}
		  });
		</script>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
	
<meta name="generator" content="Hexo 6.2.0"></head>

<body class="max-width mx-auto px3 ltr">    
      <div id="header-post">
  <a id="menu-icon" href="#" aria-label="Menu"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#" aria-label="Menu"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" aria-label="Top" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/search/">Search</a></li><!--
     --><!--
       --><li><a href="/archives/">Writing</a></li><!--
     --><!--
       --><li><a target="_blank" rel="noopener" href="https://github.com/songshuhan">Projects</a></li><!--
     -->
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" aria-label="Previous post" href="/2022/05/25/ReliableDGL/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" aria-label="Next post" href="/2022/05/23/DCRNN/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" aria-label="Back to top" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" aria-label="Share post" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://example.com/2022/05/24/word2vec/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://example.com/2022/05/24/word2vec/&text=(Google 2013) Word2vec"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://example.com/2022/05/24/word2vec/&title=(Google 2013) Word2vec"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://example.com/2022/05/24/word2vec/&is_video=false&description=(Google 2013) Word2vec"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=(Google 2013) Word2vec&body=Check out this article: http://example.com/2022/05/24/word2vec/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://example.com/2022/05/24/word2vec/&title=(Google 2013) Word2vec"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://example.com/2022/05/24/word2vec/&title=(Google 2013) Word2vec"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://example.com/2022/05/24/word2vec/&title=(Google 2013) Word2vec"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://example.com/2022/05/24/word2vec/&title=(Google 2013) Word2vec"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://example.com/2022/05/24/word2vec/&name=(Google 2013) Word2vec&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://example.com/2022/05/24/word2vec/&t=(Google 2013) Word2vec"><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AF%8D%E8%A1%A8%E5%BE%81"><span class="toc-number">1.</span> <span class="toc-text">词表征</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Word2vec"><span class="toc-number">2.</span> <span class="toc-text">Word2vec</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%A4%E4%B8%AA%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95"><span class="toc-number">3.</span> <span class="toc-text">两个优化方法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Hierarchical-Softmax"><span class="toc-number">3.1.</span> <span class="toc-text">Hierarchical Softmax</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Negative-Sampling"><span class="toc-number">3.2.</span> <span class="toc-text">Negative Sampling</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">3.3.</span> <span class="toc-text">总结</span></a></li></ol></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        (Google 2013) Word2vec
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">Shuhan Song</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2022-05-24T10:14:07.000Z" itemprop="datePublished">2022-05-24</time>
        
      
    </div>


      
    <div class="article-category">
        <i class="fas fa-archive"></i>
        <a class="category-link" href="/categories/Paper-Reading/">Paper Reading</a> › <a class="category-link" href="/categories/Paper-Reading/Natural-Language-Processing/">Natural Language Processing</a>
    </div>


      
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link-link" href="/tags/NLP/" rel="tag">NLP</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <p><strong><font color="LightCoral">paper</font></strong>:</p>
<ol>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1301.3781">https://arxiv.org/abs/1301.3781</a><br>explain: <a target="_blank" rel="noopener" href="https://so.csdn.net/so/search?q=word2vec&amp;spm=1001.2101.3001.7020">word2vec</a>的奠基性论文之一，由Google的Tomas Mikolov提出。该论文提出了CBOW和Skip-gram两种word2vec模型结构。</li>
<li><a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html">https://proceedings.neurips.cc/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html</a><br>explain: word2vec的另一篇奠基性论文。详细描述了Skip-gram模型，包括模型的具体形式和Hierarchical Softmax、Negative Sampling两种可行的训练方法。</li>
</ol>
<h1 id="词表征"><a href="#词表征" class="headerlink" title="词表征"></a>词表征</h1><ol>
<li>$one-hot$ 表征(很难表达相似度)<ol>
<li>维度太大</li>
<li>每个元素是离散的的，要不就是0，要不就是1</li>
<li>$local$ 只有一位起决定作用</li>
</ol>
</li>
<li>分布式表示<ol>
<li>维度相对较低 $d \ll \left|V\right|$</li>
<li>每个元素不再是离散的，可以是不同的实数</li>
<li>并不是只有一位起作用，联合表达一个单词</li>
</ol>
</li>
</ol>
<p>Word2vec就是一种分布式表示的方法</p>
<h1 id="Word2vec"><a href="#Word2vec" class="headerlink" title="Word2vec"></a>Word2vec</h1><p>Training Data: corpus</p>
<script type="math/tex; mode=display">
[\omega_1,\omega_2,\cdots,\omega_{t-1},\omega_t,\omega_{t+1},\cdots,\omega_T]</script><p>Target: 词的分布式表示</p>
<hr>
<p><strong><font color="DeepPink">接下来的分析过程是基于$skip-gram$,会提到三个假设：</font></strong></p>
<p>首先我们来看联合概率分布，联合概率表示为包含多个条件并且所有的条件都同时成立的概率，记作 $P(X=a,Y=b)$或 $P(a,b)$，有的书上也习惯记作$P(ab)$,<strong><font color="DeepPink">利用深度学习拟合的就是这种概率分布。</font></strong>根据公式$P(X,Y)=P(X|Y)\times P(Y)$,我们从词库中随便选择一个词，那么整个词库的概率表示应该如下所示，<strong><font color="DarkViolet">因为后面的计算会很复杂，这里为了方便计算，$\triangle$表示假设窗口大小是2C</font></strong></p>
<script type="math/tex; mode=display">
P(\omega_{1:T})= P(\omega_t) \cdot P(context(\omega_t)|\omega_t)\triangleq P(\omega_t) \cdot P(\omega_{t-c:t-1},\omega_{t+1:t+c}|\omega_t)</script><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">补充：“似然性”和“概率”（或然性）有明确的区分：概率，用于在已知一些参数的情况下，预测接下来在观测上所得到的结果；似然性，则是用于在已知某些观测所得到的结果时，对有关事物之性质的参数进行估值，也就是说已观察到某事件后，对相关母数进行猜测。</span><br></pre></td></tr></table></figure>
<p><strong>似然函数（</strong>英语：$likelihood function$）是一种关于<a target="_blank" rel="noopener" href="https://zh.m.wikipedia.org/wiki/统计模型">统计模型</a>中的<a target="_blank" rel="noopener" href="https://zh.m.wikipedia.org/wiki/母數">参数</a>的<a target="_blank" rel="noopener" href="https://zh.m.wikipedia.org/wiki/函数">函数</a>，表示模型参数中的<strong>似然性</strong></p>
<p>所以我们从上式可以写出$likelihood$：</p>
<script type="math/tex; mode=display">
P(\omega_t) \cdot P(\omega_{t-c:t-1},\omega_{t+1:t+c}|\omega_t)</script><p>又因为我们不太关注$P(\omega_t)$,因为是随机取的，而且可以取遍整个词表，有$T$个，所以我们更关注后面的条件概率</p>
<p>所以$conditional \ likelihood$可以表示为:</p>
<script type="math/tex; mode=display">
\prod_{t=1}^{T}P(\omega_{t-c:t-1},\omega_{t+1:t+c}|\omega_t)</script><p> <strong><font color="DarkViolet">这里假设每个条件概率之间是相互独立的</font></strong></p>
<p>为了化简，采用$\log$，则$average \ conditional \ likelihood$可以如下图表示：</p>
<script type="math/tex; mode=display">
\frac{1}{T} \sum_{t=1}^{T}\log P(\omega_{t-c:t-1},\omega_{t+1:t+c}|\omega_t)</script><p><strong><font color="DarkViolet">这里做第三个假设，假设内部$P(\omega_{t+i}|\omega_t)$是独立同分布的</font></strong></p>
<p>于是继续化简上式为</p>
<script type="math/tex; mode=display">
\frac{1}{T} \sum_{t=1}^{T}\log \prod_{i \in[-c,0) \cup (0,c]  }P(\omega_{t+i}|\omega_t) =  \frac{1}{T} \sum_{t=1}^{T} \sum_{i \in[-c,0) \cup (0,c]  } \log P(\omega_{t+i}|\omega_t)</script><p>到现在可以看出只要关注给定一个中心词，该中心词窗口内其他词的概率就可以，<strong><font color="DarkViolet">注意，这里窗口内的词都是独立的，不存在顺序问题(比如要先出现$\omega_{t}$再出现$\omega_{t+1}$)，这样的表示已经忽略了次序，这里都是独立的。</font></strong></p>
<hr>
<p><strong><font color="LightCoral">当然，从上面的分析我们暂时看不出来和求词表分布式表示有啥关系？接下来我们探索这个问题</font></strong></p>
<p>首先我们从输入输出和模型的角度来分析，先看下面这张图</p>
<p><br></p>
<p><img src="/2022/05/24/word2vec/image-20220524213551921.png" alt="image-20220524213551921"></p>
<p><br></p>
<p>首先我们选取一个中心词$\omega_i$，用$one-hot$的表示输入的话则是一个$\mathbb R^{|V| \times 1}$的向量，其中$|V|$是词表中所有单词的数量，这个向量只有在第$i$位是1，其他位全都为0。参数矩阵$W \in \mathbb R^{|V| \times d}$和$U \in \mathbb R^{d \times |V|} $，<strong><font color="DarkViolet">注意这里的$W$矩阵没有使用激活层，是线性的，为了计算简单，如果用矩阵相乘的方式，那么用$\mathbb R^{|V| \times 1}$的向量转置以后得到的$\mathbb R^{ 1 \times |V| }$向量作为输入，在模型的输出也会得到一个$\mathbb R^{ 1 \times |V| }$的向量</font></strong>，这里我们要特别理解模型的输出代表什么，如下所示：</p>
<script type="math/tex; mode=display">
\mathbb R^{ 1 \times |V| } \stackrel{每一位的表示}\longrightarrow \left\{ P(\omega_1|\omega_i),P(\omega_2|\omega_i),P(\omega_3|\omega_i),\cdots,P(\omega_{|V|}|\omega_i)\right\}</script><p>然后使用softmax将所有概率值放在$(0，1]$区间当中，即对于中心词$\omega_{i}$，所预测的其他所有词的概率，令$softmax$之后的值为，</p>
<script type="math/tex; mode=display">
\left\{ \hat P(\omega_1|\omega_i),\hat P(\omega_2|\omega_i),\hat P(\omega_3|\omega_i),\cdots,\hat P(\omega_{|V|}|\omega_i)\right\}</script><p>对于中心词$\omega_{i}$，所预测的其他所有词的概率之和为1：</p>
<script type="math/tex; mode=display">
\sum_{t=1}^{|V|} \hat P(\omega_t|\omega_i)=1</script><p><img src="/2022/05/24/word2vec/image-20220524233941683.png" alt="image-20220524233941683"></p>
<p><br></p>
<p>当然我们需要在$\omega_{i}$长度为$2c$的窗口中每个上下文一个位置所有的概率值，注意下图对应的$W^{‘}$参数矩阵就是上图的$U$矩阵(注意，不是多个不同的U矩阵，都是同一个，这里为了形象化不同的上下文位置拆开了)，这里我理解假如一共有5个词，s上下文窗口有两个词对应的索引分别式1和3，则所代表的$\hat y_{target}=[0,1,0,1,0]$，在损失函数优化的时候应该用$\hat y_{target}-y_{output}$合在一起算，下面的分开算只是为了好理解，其前面的c个词和后面的c个词作为了$Skip-Gram$模型的输出,，期望这些词的$softmax$概率比其他的词大。</p>
<p><br></p>
<p><img src="/2022/05/24/word2vec/20200831235730940.png" alt></p>
<p><br></p>
<p>最后我们讨论损失函数，我们已经有了上面的似然函数，则损失函数的优化可以如下表示，因为我们要最大化中心词和上下文之间的联系概率，所以要</p>
<script type="math/tex; mode=display">
\max \frac{1}{T} \sum_{t=1}^{T} \sum_{i \in[-c,0) \cup (0,c]  } \log P(\omega_{t+i}|\omega_t)</script><p>对上式我们取负数，则变成了最小化：</p>
<script type="math/tex; mode=display">
\min -\frac{1}{T} \sum_{t=1}^{T} \sum_{i \in[-c,0) \cup (0,c]  } \log P(\omega_{t+i}|\omega_t)</script><p>所以损失函数即为：</p>
<script type="math/tex; mode=display">
J(\theta)=-\frac{1}{T} \sum_{t=1}^{T} \sum_{i \in[-c,0) \cup (0,c]  } \log P(\omega_{t+i}|\omega_t)</script><p><strong><font color="Crimson">那最后该怎么得到所有词的向量呢？我们在训练的过程中不断的更新$W$和$U$的参数矩阵，在训练完成之后，用$one-hot$向量和训练好的$W$矩阵相乘就是得到的词向量了，也就是词的embedding！！！（注意，不需要$U$矩阵）</font></strong></p>
<p>最后</p>
<p><strong><font color="LightCoral">这里skip-gram的介绍就完成了，CBOW的思想类似</font></strong></p>
<h1 id="两个优化方法"><a href="#两个优化方法" class="headerlink" title="两个优化方法"></a>两个优化方法</h1><h2 id="Hierarchical-Softmax"><a href="#Hierarchical-Softmax" class="headerlink" title="Hierarchical Softmax"></a>Hierarchical Softmax</h2><p><strong><font color="MediumBlue">思考：如果词库特别大，也就是$|V|$特别大，那在最后计算$softmax$的计算量也会非常大，怎么办呢？</font></strong></p>
<p>该方法最后一层的长度不设置成$|V|$大小，而是设置成一棵树的节点数的长度，这里首先要了解$Huffman$树的构造方法和$Huffman$编码。</p>
<p><strong><font color="DarkViolet">这里看这个图，非常容易理解，按照词频进行节点的融合，越靠近根节点的词频越大，黄色节点是在构成树的过程当中添加的节点，注意，所有词都在叶子节点上，但非叶子节点上也要定义和叶子节点等长的向量，为了方便计算</font></strong></p>
<p><img src="/2022/05/24/word2vec/image-20220525010308984.png" alt="image-20220525010308984"></p>
<p><br></p>
<p><strong><font color="LightSeaGreen" size="4">$CBOW$</font></strong></p>
<p><br></p>
<p>在得到了上面的图之后，我们就可以来计算概率，首先以$CBOW$为例子，将输入层的$2c$个词向量累加求和，即</p>
<script type="math/tex; mode=display">
X_w=\sum_{i=1}^{2c}v(context(w_i)) \quad v(context(w_i)\in \mathbb R^{m}</script><p> <strong><font color="DarkViolet">这里一定要注意，一开始死活不明白为什么用二分类乘积的方法就可以计算$w$在其上下文的条件概率，就是因为忽略了这里，其实$X_w$在二分类中已经被当作条件使用了，太蠢了，这点无知错误都没想明白</font></strong></p>
<p><img src="https://img-blog.csdnimg.cn/20200603202110178.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1ZhcmlhYmxlWA==,size_16,color_FFFFFF,t_70#pic_center" alt="img"></p>
<p><img src="/2022/05/24/word2vec/image-20220525011220421.png" alt="image-20220525011220421"></p>
<p><br></p>
<p><strong><font color="LightCoral" size="4">在得到了一个词在其上下文的概率之后，就可以扩展到整个词库，从而得到损失函数，采用随机梯度上升法将这个函数最大化。</font></strong></p>
<p><br></p>
<p><img src="/2022/05/24/word2vec/image-20220525012323049.png" alt="image-20220525012323049"></p>
<p><br></p>
<p><strong><font color="LightSeaGreen" size="4">$Skip-gram$</font></strong></p>
<p><br></p>
<p>讲完了$CBOW$，其实$Skip-gram$的思想一模一样，就是反过来了,之前的$CBOW$是根据上下文预测中间词，而$Skip-gram$是根据中间词预测上下文，即有了$X_w$来得到$p(context(w)|w)$，根据这个思想得到优化函数，其实就是要对$context(w)$的所有词概率进行相乘，和$CBOW$是反的，可以理解为一个是多对一，一个是一对多，但本质上都是为了让$w$和$context(w)$联系的更加紧密</p>
<p><br></p>
<p><img src="https://img-blog.csdnimg.cn/20200603202953553.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1ZhcmlhYmxlWA==,size_16,color_FFFFFF,t_70#pic_center" alt="img"></p>
<p><img src="/2022/05/24/word2vec/image-20220525013057616.png" alt="image-20220525013057616"></p>
<p><br></p>
<p>同样，采用随机梯度上升法将这个函数最大化。</p>
<hr>
<h2 id="Negative-Sampling"><a href="#Negative-Sampling" class="headerlink" title="Negative Sampling"></a>Negative Sampling</h2><p><img src="/2022/05/24/word2vec/image-20220525014133991.png" alt="image-20220525014133991"></p>
<p><br></p>
<p><strong><font color="LightSeaGreen" size="4">$CBOW$</font></strong></p>
<p><br></p>
<p><img src="/2022/05/24/word2vec/image-20220525014312156.png" alt="image-20220525014312156"></p>
<p><img src="/2022/05/24/word2vec/image-20220525014343905.png" alt="image-20220525014343905"></p>
<p><br></p>
<p><strong><font color="LightSeaGreen" size="4">$Skip-gram$</font></strong></p>
<p><br></p>
<p><img src="/2022/05/24/word2vec/image-20220525014705835.png" alt="image-20220525014705835"></p>
<p><img src="/2022/05/24/word2vec/image-20220525014728040.png" alt="image-20220525014728040"></p>
<p><br></p>
<p><strong><font color="LightSeaGreen" size="4">如何负采样</font></strong></p>
<p><br></p>
<p>上面个介绍了CBOW和skip-gram的负采样方法，那刚才的遗留问题，究竟怎样负采样呢？</p>
<p><img src="/2022/05/24/word2vec/image-20220525015241713.png" alt="image-20220525015241713"></p>
<p><img src="/2022/05/24/word2vec/image-20220525015246285.png" alt="image-20220525015246285"></p>
<p><br></p>
<p>参考资料：</p>
<ol>
<li><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/peghoty/p/3857839.html">https://www.cnblogs.com/peghoty/p/3857839.html</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/VariableX/article/details/106531987">https://blog.csdn.net/VariableX/article/details/106531987</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1hy4y1n7ik?p=3&amp;spm_id_from=pageDriver">https://www.bilibili.com/video/BV1hy4y1n7ik?p=3&amp;spm_id_from=pageDriver</a></p>
<p>explain:白板推导系列很不错</p>
</li>
</ol>
<hr>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><strong><font color="LightCoral" size="4">这一次下了比较大的功夫，终于对word2vec有一个比较深入的理解了，之前总是看了一知半解，不够认真，学东西的时候一定更沉下心，要不就是在浪费时间，原理明白了，但是对代码的实现还差的很远，继续加油吧。</font></strong></p>

  </div>
</article>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/search/">Search</a></li>
         
          <li><a href="/archives/">Writing</a></li>
         
          <li><a target="_blank" rel="noopener" href="https://github.com/songshuhan">Projects</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AF%8D%E8%A1%A8%E5%BE%81"><span class="toc-number">1.</span> <span class="toc-text">词表征</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Word2vec"><span class="toc-number">2.</span> <span class="toc-text">Word2vec</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%A4%E4%B8%AA%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95"><span class="toc-number">3.</span> <span class="toc-text">两个优化方法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Hierarchical-Softmax"><span class="toc-number">3.1.</span> <span class="toc-text">Hierarchical Softmax</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Negative-Sampling"><span class="toc-number">3.2.</span> <span class="toc-text">Negative Sampling</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">3.3.</span> <span class="toc-text">总结</span></a></li></ol></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://example.com/2022/05/24/word2vec/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://example.com/2022/05/24/word2vec/&text=(Google 2013) Word2vec"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://example.com/2022/05/24/word2vec/&title=(Google 2013) Word2vec"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://example.com/2022/05/24/word2vec/&is_video=false&description=(Google 2013) Word2vec"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=(Google 2013) Word2vec&body=Check out this article: http://example.com/2022/05/24/word2vec/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://example.com/2022/05/24/word2vec/&title=(Google 2013) Word2vec"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://example.com/2022/05/24/word2vec/&title=(Google 2013) Word2vec"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://example.com/2022/05/24/word2vec/&title=(Google 2013) Word2vec"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://example.com/2022/05/24/word2vec/&title=(Google 2013) Word2vec"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://example.com/2022/05/24/word2vec/&name=(Google 2013) Word2vec&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://example.com/2022/05/24/word2vec/&t=(Google 2013) Word2vec"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2022-Forever
    Shuhan Song
  </div>
  <div class="footer-right">
    <nav>
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/search/">Search</a></li><!--
     --><!--
       --><li><a href="/archives/">Writing</a></li><!--
     --><!--
       --><li><a target="_blank" rel="noopener" href="https://github.com/songshuhan">Projects</a></li><!--
     -->
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->



  <link rel="preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.2/css/all.min.css" crossorigin="anonymous" onload="this.onload=null;this.rel='stylesheet'"/>


    <!-- jquery -->
 
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" crossorigin="anonymous"></script> 




<!-- clipboard -->

  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.7/clipboard.min.js" crossorigin="anonymous"></script> 
  
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"Copy to clipboard!\">";
    btn += '<i class="far fa-clone"></i>';
    btn += '</span>'; 
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "Copied!");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Google Analytics -->

<!-- Baidu Analytics -->

<!-- Cloudflare Analytics -->

<!-- Umami Analytics -->

<!-- Disqus Comments -->

<!-- utterances Comments -->

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

</body>
</html>
